model_name: model_gpt_small
# --- Architecture ---
num_heads: 12
num_layers: 12
embed_dim: 768
ff_hidden_dim: 3072
max_length: 1024
vocab_size: 50257
output_dim: 50257
# --- Regularization ---
qkv_bias: false
mha_dropout_rate: 0.1
ffn_dropout_rate: 0.1
emb_dropout_rate: 0.1
layer_norm_eps: 0.00001
# --- Optimization ---
learning_rate: 0.0004      
weight_decay: 0.1
min_lr: 4e-5               
warmup_steps: 500         
# --- Training Loop Dynamics ---
micro_batch_size: 8       
grad_accumulation: 16 
num_epochs: 1
# --- Checkpointing & Eval ---
eval_freq: 500             
eval_iter: 20
temperature: 1.0
top_k: 40
top_p: 0.95
use_checkpoint: last
checkpoint_path: checkpoint